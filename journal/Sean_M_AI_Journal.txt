AI Usage Journal
Sean McLean
smclean2@umd.edu
Section 0203

Week 2, Entry #1
This week I made some usage of AI during my coding for the first time this semester. As I was completing the weekly exercises, I came across a question that was unfamiliar (print a fixed width label so that words align, this ultimately ended up being accomplished using the f-format feature). I was not aware that this was even a feature in python, as had I ever seen width mentioned with regards to printing (this material wasn't covered in my 126 section last semester), so I prompted perplexity AI to provide me code that allows printed statements to be adjusted to a set width, and the code it returned was : print(f"Title: {title:<30}  Author: {author:<20}") , which signaled to me that I needed to be using the f formatter and adding the :<num after each variable encased in the variable brackets. After this I coded my final solution with different numbers, and noted in my notebook that this was a feature you could use when f-formatting.

Week 3, Entry #2
This week I made a couple more minor usages of AI in my weekly exercises. The first instance was when I couldnt recall which function to use to find the index of characters in a string, so I looked up what that function was, and Google Gemini returned the result that there are two functions (.index() for lists, .find() for strings). Once I remembered that I was able to advance quickly through the exercises, until I reached the later exercise involving the enforcement of the title casing, where I couldnt remember that function either, which I again prompted gemini to provide me with (its the string.title() function), before quickly implementing the function in my response to the question. I probably need to freshen up on my functions knowledge a little bit, remembering what does what.

Week 4, Entry # 3
This week I again made some small references to AI in my weekly exercises. The one I remember the most was while I was attempting the first exercise on the sheet (the normalize title one), I at first implemented a very long solution that spanned several lines of code. Since the exercises are meant to be programmed with simplicity, I could not help but wonder if I was forgetting something, so I pasted my solution into perplexity AI and asked if there were any built in functions that could simplify my code. It told me to consider implementing the "'string.join" function, which given a list of strings inserts a string in between each entry and concatenates everything together. So I thought to make the string a space, and after combining it with the split and title functions, my solution was reduced to just a single return statement. Later on in the exercises I was stuck on the final function I had not gotten around to, which was the starts with prefix exercise, and so I asked perplexity AI if python has a built in function to deal with this sort of thing, where it reminded me of the .startswith() function, which I ultimately implemented (I find it odd I couldn't remember this function, as it appeared very frequently in my INST126 section last semester).

Week 5, Entry #4
This week I used very few references to AI in the weekly exercises. The first that I recall was to find out how to initialize the set, since I had no familiarity with this concept from any of the previous lectures or classes. I thought I could create it by setting it equal to {} at first and that the type would auto update once I started applying functions to the variable, but this of course didn't work, so I prompted Google Gemini to tell me what statement initializes sets in python, to which it returned the set() function which quickly fixed my code once implemented (I also prompted Gemini to list whether or not sets used the append function to add items, to which it explained the add function is used instead). Very early on I also prompted Gemini to tell me what the pass statements were about, since I couldn't quite grasp why they were in every exercise, to which it explained they are simply placeholder statements which I was free to remove after implementing code, which I did. Beyond what I've listed, I made it through all exercises without using AI assistance, which is promising to me.

Week 6, Entry #5
This week I used AI a few times to debug my code in the weekly exercises. This weeks exercises were the most confusing so far, so I used Perplexity AI a couple more time than I did last week. The first that comes to mind was with the 7th exercise, specifically because it asked to return a tuple. The class in question only had one attribute for its objects, which was a list, and yet the instructions specified to return a tuple with the @property function for that attribute. This made no sense to me since there were not 2 distinct objects to place into one tuple and return, so after several failed attempts I asked perplexity AI to explain what the question prompt meant. It returned an explanation to me which stated to simply cast the attribute to a tuple via tuple(), and everything should compile. I adjusted the @ property as such, and the getter method thereafter returned tuples that did not have two distinct objects, but had one object, a comma, and nothing. I did not know tuples could come out like this and thought they could only be returned/printed if they contained 2 distinct values or objects. I used it in the 10th exercises as well when a bug occurred where I was trying to use the isdigit() function on integers, only to find out from perplexity that isdigit() is string specific. Later on in the exercise I needed the functionality to check the data type of an object in order to solve the problem, so I asked perplexity if a function existed for this, to which it provided the isinstance() function, which I found very helpful and I am glad to be aware of now. I am definitely noticing a pattern now that one of the best uses of AI is to become aware of functions quickly without having to waste time searching forums and overflow websites.

Week 7, Entry #6
The previous iteration of this entry was an error (I was confusing myself with another recent coding experience where I was using very little AI). For the Week 6 INST326 Exercises, I tackled a new, admittedly more involved approach to incorporating AI in my coding process. After completing the first five exercises as normal, I realized I was taking a pretty unsatisfactory amount of time to complete each exercise due to repeatedly writing out skeletons of the problem based on already vidid enough problem descriptions (time wasted manually writing out the listed class names and function headers). So for each remaining problem, I asked Perplexity AI to generate a "skeleton" of the problem description, which auto generated any class names or function/method headers explicitly mentioned in the problem description WITHOUT generating any of the body code for the functions/methods themselves, while also generating short, basic docstrings for those functions/methods. I found this prerferrable to manually writing out explicitly listed class names and method headers manually in repitition since it abstracted away from the purpose of the work (learn how to think and solve problems by writing efficient body code). So for this assignment, most class names, function HEADERS, and docstrings are auto generated. For an example of this, exercise 7 states to write "Class method format_isbn(isbn)" a five-word description which instantly implies four different things (function decorator of @classmethod, name of format_isbn, parameter one of cls for a class method, and added parameter of isbn as listed). With these things explicitly implied to the human, the AI can recognize that as well and auto generate @classmethod def format_isbn(cls, isbn): which meets all the listed implications and adds a sensible doctring, """Return ISBN with dashes removed.""" based on the description, which is what it did for exercise 7 as well as many others. A similar process followed for exercises explicitly requesting classes containing instance methods or static methods, which both imply by nature the class name, method decorator, method name, and method paramerters (so for requested classes with instance methods, its implied theres a listed class name, zero decorator, a listed method name, and method parameters of "self" first plus any other extras listed). As such, quick, efficient class/method headers and relevant docstrings were auto generated by Perplexity for most exericses. Just as importantly, all body code inside of all the methods, including __init__ methods, remained written manually by me. The one exception I remember was exercise 18, which requested to create factory methods for a "Config" class where each "Config" object had an attribute of a URL, which I obviously didnt know by heart, so for which I asked perplexity to generate some URLs relevant to the problem I could use to complete the factory methods (I still came up with the env_name and feature_flags attributes of each of those 3 factory constructors myself).

Week 8, Entry #7
This week I used a similar approach to last week in that I prompted Perplexity AI to generate a "skeleton" of the problem description each time so that each question had all its explicitly listed functions/method headers/type hints auto generated for each problem so I could immediate begin the work of writing out the body code and running any necessary tests without taking time to manually copy/paste this information each time. Beyond this, I used Perplexity to occasionally remind me of some functions that I needed to use to complete for the exercises (ex: I did not remember the .get() structure for dictionaries which was required for one of the later exercises, so I prompted perplexity to explain how I retrieve entries from the dictionary to which it reminded me of this function and more importantly reminded me of the failsafe value, the 2nd parameter, that the function uses when theres no information to retrieve from the dictionary). Beyond framing out method headers/type hints and occasional reminders of pythons built-in functions that are needed for problem solutions, this week went relatively straight forward and the AI was more a tool for helping this week than a crutch.